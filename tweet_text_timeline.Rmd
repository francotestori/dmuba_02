---
title: "Tweet Time Words"
output: html_notebook
---

Carga inicial del dataframe de tweets COVID-19
```{r}
library(mongolite)
library(ggplot2)
library(dplyr)
library(tidyr)
library(tidytext)
library(dplyr)
library(tm, exclude = "inspect")
library(stringr)
library(stringi)
library(arules)
library(tokenizers)


tweets_mongo_covid19 <- mongo(
  collection = "tweets_mongo_covid19", 
  db = "DMUBA"
)
```



# Exploramos el uso de hashtags
Ya teniendo una primera impresión de la evolución de los tweets en base sus fechas, exploramos el uso de hashtags
```{r}
tweet_text = tweets_mongo_covid19$aggregate(
'[
    {
        "$match": {}
    },
    {
        "$project": {
            "status_id": 1,
            "verified": 1,
            "location": 1,
            "source": 1,
            "created_at": {
                "$dateToString": { "date": "$created_at"}
            },
            "favorite_count": 1,
            "retweet_count": 1,
            "text": 1,

            "retweet_status_id": 1,
            "retweet_verified": 1,
            "retweet_location":1,
            "retweet_source": 1,
            "retweet_created_at": {
                "$cond": { 
                    "if": { 
                        "$eq" : ["$retweet_created_at", {}] 
                    }, 
                    "then": null, 
                    "else": {
                        "$dateToString": {"date": "$retweet_created_at"}
                    }
                }
            },
            "retweet_favorite_count": 1,
            "retweet_retweet_count": 1,
            "retweet_text": 1,

            "quoted_status_id": 1,
            "quoted_verified": 1,
            "quoted_location": 1,
            "quoted_source": 1, 
             "quoted_created_at": {
                "$cond": { 
                    "if": { 
                        "$eq" : ["$quoted_created_at", {}] 
                    }, 
                    "then": null, 
                    "else": {
                        "$dateToString": {"date": "$quoted_created_at"}
                    }
                }
            },
            "quoted_favorite_count": 1,
            "quoted_retweet_count": 1,
            "quoted_text": 1
        }
    }
]'
)
```

```{r}
original_tweet_headers <- c(
  'status_id',
  'verified',
  'location',
  'source',
  'created_at',
  'favorite_count',
  'retweet_count',
  'text'
)
original_tweets = tweet_text[
  is.na(tweet_text$retweet_text) & is.na(tweet_text$quoted_text),
  original_tweet_headers
]

# Retweets
retweeted_tweet_headers <- c(
  'retweet_status_id',
  'retweet_verified',
  'retweet_location',
  'retweet_source',
  'retweet_created_at',
  'retweet_favorite_count',
  'retweet_retweet_count',
  'retweet_text'
)

retweeted_tweets = tweet_text[,retweeted_tweet_headers]

names(retweeted_tweets)[names(retweeted_tweets) == 'retweet_status_id'] <- 'status_id'
names(retweeted_tweets)[names(retweeted_tweets) == 'retweet_verified'] <- 'verified'
names(retweeted_tweets)[names(retweeted_tweets) == 'retweet_location'] <- 'location'
names(retweeted_tweets)[names(retweeted_tweets) == 'retweet_source'] <- 'source'
names(retweeted_tweets)[names(retweeted_tweets) == 'retweet_created_at'] <- 'created_at'
names(retweeted_tweets)[names(retweeted_tweets) == 'retweet_favorite_count'] <- 'favorite_count'
names(retweeted_tweets)[names(retweeted_tweets) == 'retweet_retweet_count'] <- 'retweet_count'
names(retweeted_tweets)[names(retweeted_tweets) == 'retweet_text'] <- 'text'

# Quotes
quoted_tweet_headers <- c(
  'quoted_status_id',
  'quoted_verified',
  'quoted_location',
  'quoted_source',
  'quoted_created_at',
  'quoted_favorite_count',
  'quoted_retweet_count',
  'quoted_text'
)
quoted_tweets = tweet_text[,quoted_tweet_headers]

names(quoted_tweets)[names(quoted_tweets) == 'quoted_status_id'] <- 'status_id'
names(quoted_tweets)[names(quoted_tweets) == 'quoted_verified'] <- 'verified'
names(quoted_tweets)[names(quoted_tweets) == 'quoted_location'] <- 'location'
names(quoted_tweets)[names(quoted_tweets) == 'quoted_source'] <- 'source'
names(quoted_tweets)[names(quoted_tweets) == 'quoted_created_at'] <- 'created_at'
names(quoted_tweets)[names(quoted_tweets) == 'quoted_favorite_count'] <- 'favorite_count'
names(quoted_tweets)[names(quoted_tweets) == 'quoted_retweet_count'] <- 'retweet_count'
names(quoted_tweets)[names(quoted_tweets) == 'quoted_text'] <- 'text'
```

Combinamos nuestros tweets y aplicamos transformaciones a las fechas
```{r}
combined_tweets = rbind(original_tweets, retweeted_tweets, quoted_tweets)

combined_tweets['tweet_date'] = as.POSIXct(
  combined_tweets$created_at, 
  format="%Y-%m-%dT", 
  tz="UTC"
)

combined_tweets['created_at'] = as.POSIXct(
  combined_tweets$created_at, 
  format="%Y-%m-%dT%H:%M:%S", 
  tz="UTC"
)

# Limpiamos duplicados
combined_tweets = combined_tweets[!duplicated(combined_tweets),]
combined_tweets[!is.na(combined_tweets['tweet_date']),]
```

Hacemos un tratamiento de textos
```{r}
### ----- Tratamiento de Textos ------------
df_text = combined_tweets[,c("status_id", "source", "location", "tweet_date", "text")]
```

Limpiamos caracteres no alfanumericos
```{r}
# Se quitan caracteres no alfanumÃ©ricos (por cuestiones de errores en RStudio)
df_text$text <- gsub("[^[:alnum:][:blank:]?&/\\-]", "", df_text$text)
df_text$text <- gsub("U00..", "", df_text$text)
```

Limpiamos los textos
```{r}
# Se quitan acentos
df_text$text = stri_trans_general(df_text$text, "Latin-ASCII")
# Se pasa a minusculas
df_text$text = tolower(df_text$text)
# Se quita puntuacion
df_text$text = removePunctuation(df_text$text)
# Se quitan nÃºmeros
df_text$text = removeNumbers(df_text$text)
# se quitan espacios extras
df_text$text =  stripWhitespace(df_text$text)
# se quitan espacios al principio y final de la cadena
df_text$text = str_trim(df_text$text)
# sin stop words
df_text$text = removeWords(df_text$text, stopwords("spanish"))
# sin stop words english
df_text$text = removeWords(df_text$text, stopwords("english"))
```

Tokenizamos el texto (separamos todos los terminos)
```{r}
# se separa el texto en terminos
df_text$words = tokenizers::tokenize_words(df_text$text, simplify = T)
```

Armamos un dataframe con las palabras utilizadas en cada tweet por día
```{r}
# se pasa a formato pares: tweet-words
df_words_with_date = df_text %>% select("status_id", "tweet_date", "words")  %>% unnest(words) %>%  distinct()
```

Contamos las palabras por fecha filtrando por terminologia que no queremos
```{r}
words_counts_by_date <- df_words_with_date %>%
  filter(
    !str_detect(str_to_lower(words), str_to_lower(".*COVID.*|.*coronavirus.*|.*cuarentena*"))
  ) %>%
  group_by(tweet_date, words) %>%
  count(words)
  

total_words <- words_counts_by_date %>% 
  group_by(words) %>% 
  summarize(total = sum(n))

words_counts_by_date <- left_join(words_counts_by_date, total_words)

words_counts_by_date <- words_counts_by_date %>%
    bind_tf_idf(words, tweet_date, n)

words_counts_by_date = as.data.frame(
  words_counts_by_date
)

names(words_counts_by_date)[3] = 'count'

# Elimino todos los tweets sin hashtags (NA)
words_counts_by_date = words_counts_by_date[!is.na(words_counts_by_date['words']),]
```

Aplicamos el scoring de ocurrencias de palabras por día sobre el total de incidencias para determinar su peso.
```{r}
# Filtramos palabras
selected_words = words_counts_by_date[words_counts_by_date$count >= 20,]

# No filtramos por count
#selected_words = words_counts_by_date

selected_words = selected_words[order(selected_words$tweet_date, selected_words$tf_idf, decreasing = TRUE),]

selected_words <-
  selected_words %>%
    group_by(tweet_date) %>%
    top_n(10, tf_idf)
```

Agregamos el prefijo de de tipo de atributo (word)
```{r}
df_words = selected_words[,c("tweet_date","words","tf_idf")]

# se agrega prefijo de tipo de Ã�tem:
#df_words$words = paste0("word=", df_words$words)
```


Discretizamos el valor de tf_idf
```{r}
df_words$words_impact = discretize(
  df_words$tf_idf,
  method = "fixed",
  breaks = c(0 , 0.002955 , 0.004168 , Inf),
  labels = c("pocos", "medio", "mucho")
)
```

Generamos nuestras transacciones
```{r}
# reglas
words_impact_transactions <- as(
  df_words[
    c(
    "tweet_date",
    "words",
    "words_impact"
    )
  ], 
  "transactions"
)

print(words_impact_transactions)
```

Generamos nuestras reglas
```{r}
words_impact_rules = apriori(
  words_impact_transactions, 
  parameter = list(
    target="rule", 
    support=0.01, 
    confidence=0.02
  )
)

print(words_impact_rules)

inspect(
  sort(
    words_impact_rules,
    by="lift", 
    decreasing = TRUE
  )
)
```

Reglas para palabras ordenadas por lift
```{r}
inspect(
  sort(
    subset(words_impact_rules, subset = lhs  %pin% "words="),
    by="lift", 
    decreasing = TRUE
  )
)
```

Reglas en base al impacto medio/mucho
```{r}
inspect(
  sort(
    subset(words_impact_rules, subset = lhs  %pin%  "words_impact=medio" | lhs  %pin%  "words_impact=mucho"),
    by="lift", 
    decreasing = TRUE
  )
)
```

Reglas que generan impacto medio/mucho
```{r}
inspect(
  sort(
    subset(words_impact_rules, subset = rhs  %pin%  "words_impact=medio" | rhs  %pin%  "words_impact=mucho"),
    by="lift", 
    decreasing = TRUE
  )
)
```

Ahora vamos a analizar si mis tweets utilizaron el lenguaje del día 
Tomamos como lenguaje del día (diccionario) las "selected_words"
```{r}
dictionary = selected_words

# se pasa a formato pares: tweet-words
df_dictionary = df_text %>% 
  select("status_id", "tweet_date", "words", "location", "source")  %>% 
  unnest(words) %>%  
  distinct() %>%
  filter(
    !str_detect(str_to_lower(words), str_to_lower(".*COVID.*|.*coronavirus.*|.*cuarentena*"))
  )

dates = unique(dictionary$tweet_date)
```

Para todas las fechas que figuran en mi diccionario, me fijo si el termino en el tweet (trx) 
uso el vocabulario del día
```{r}

df_dictionary$use_date_dictionary = FALSE

for (date in dates) {
    date_dictionary = as.list(dictionary[dictionary$tweet_date == date, "words"]$words)
    
    df_dictionary[
      df_dictionary$tweet_date == date & df_dictionary$words %in% date_dictionary,
      "use_date_dictionary"
    ]$use_date_dictionary <- TRUE
    
    remove(date_dictionary)
}

df_dictionary$use_date_dictionary = ifelse(df_dictionary$use_date_dictionary == FALSE, NA, "S")
```

Transformamos las sources de nuestros tweets filtrando como conocidas aquellas 
que fueron utilizadas en mas de 5000 tweets
```{r}
known_sources <- df_dictionary %>%
  group_by(source) %>%
  count(source) %>%
  filter(n > 5000)

df_dictionary$source = ifelse(df_dictionary$source %in% as.list(known_sources$source), df_dictionary$source, NA) 
```


Generamos nuestras transacciones
```{r}
# reglas
dictionary_transactions <- as(
  df_dictionary[
    c(
    "status_id",
    "tweet_date",
    "words",
    "source",
    "use_date_dictionary"
    )
  ], 
  "transactions"
)

print(dictionary_transactions)
```

Generamos nuestras reglas
```{r}
dictionary_rules = apriori(
  dictionary_transactions, 
  parameter = list(
    target="rule", 
    support=0.01, 
    confidence=0.02
  )
)

print(dictionary_rules)

inspect(
  sort(
    dictionary_rules,
    by="lift", 
    decreasing = TRUE
  )
)
```


```{r}
View(     
)
```

